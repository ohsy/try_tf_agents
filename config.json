{    
    "dtype" : "float32",
    "intdtype" : "int32",
    "environment" : "CartPole-v1",
    "agent" : "DQN",
    "resultPath" : "./result",
    "logDirname" : "log",
    "modelDirname" : "model",
    "isGPUUsed" : true,

    "num_train_steps" : 5000, 
    "num_train_steps_to_log" : 20,
    "num_train_steps_to_eval" : 100, 
    "num_train_steps_to_save_policy" : 5000,
    "num_episodes_to_eval" : 10,

    "qnet_fc_layer_params" : (128, 64),
    "actor_fc_layer_params" : (256, 256),
    "# actor_fc_layer_params" : "(32, 32) for Pendulum",
    "critic_observation_fc_layer_params" : None,
    "#critic_observation_fc_layer_params" : (64, 64),
    "# critic_observation_fc_layer_params" : "(32, 32) for Pendulum",
    "critic_action_fc_layer_params" : None,
    "#critic_action_fc_layer_params" : (64, 64),
    "# critic_action_fc_layer_params" : "(32, 32) for Pendulum",
    "critic_joint_fc_layer_params" : (256, 256),
    "# critic_joint_fc_layer_params" : "(128, 16) for Pendulum",
    "value_fc_layer_params" : (256, 256),

    "batch_size" : 64, 
    "learning_rate" : 1e-3,
    "actor_learning_rate" : 3e-4,
    "critic_learning_rate" : 6e-4,
    "alpha_learning_rate" : 3e-4,
    "target_update_tau" : 0.005,
    "target_update_period" : 1, 
    "gamma" : 0.99, 
    "reward_scale_factor" : 1.0,

    "replay_buffer_max_length" : 500,
    "# num_frames = capacity = max_length * env.batch_size and default env.batch_size = 1" : None, 
    "num_initial_collect_steps" : 100,
    "num_collect_steps_per_train_step" : 1,

    "# for PPO" : None,
    "num_parallel_envs" : 30, 
    "num_env_steps" : 20000,
    "num_epochs" : 25,
    "num_collect_episodes_per_iteration" : 30,

    "#  ending": None
}

